{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hosen42/text-classification-with-transformer?scriptVersionId=143592750\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"JaS_Kov-5xqJ"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"id":"CJEjVg4k5xqJ","execution":{"iopub.status.busy":"2023-09-20T03:58:23.48834Z","iopub.execute_input":"2023-09-20T03:58:23.48985Z","iopub.status.idle":"2023-09-20T03:58:33.190091Z","shell.execute_reply.started":"2023-09-20T03:58:23.489814Z","shell.execute_reply":"2023-09-20T03:58:33.188692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implement a Transformer block as a layer","metadata":{"id":"hrQzzQAl5xqL"}},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"id":"mYeqU_2I5xqL","execution":{"iopub.status.busy":"2023-09-20T03:58:33.193057Z","iopub.execute_input":"2023-09-20T03:58:33.194553Z","iopub.status.idle":"2023-09-20T03:58:33.204146Z","shell.execute_reply.started":"2023-09-20T03:58:33.194513Z","shell.execute_reply":"2023-09-20T03:58:33.202945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implement embedding layer\n\nTwo seperate embedding layers, one for tokens, one for token index (positions).","metadata":{"id":"-7WBs8hY5xqM"}},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"id":"792p8TkZ5xqM","execution":{"iopub.status.busy":"2023-09-20T03:58:33.205768Z","iopub.execute_input":"2023-09-20T03:58:33.206235Z","iopub.status.idle":"2023-09-20T03:58:33.218064Z","shell.execute_reply.started":"2023-09-20T03:58:33.206175Z","shell.execute_reply":"2023-09-20T03:58:33.217019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download and prepare dataset","metadata":{"id":"V7vp8Td85xqN"}},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 200  # Only consider the first 200 words of each movie review\n(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\nprint(len(x_train), \"Training sequences\")\nprint(len(x_val), \"Validation sequences\")\nx_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\nx_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)","metadata":{"id":"WYmCUEi-5xqN","execution":{"iopub.status.busy":"2023-09-20T03:58:33.220225Z","iopub.execute_input":"2023-09-20T03:58:33.220753Z","iopub.status.idle":"2023-09-20T03:58:39.624684Z","shell.execute_reply.started":"2023-09-20T03:58:33.220719Z","shell.execute_reply":"2023-09-20T03:58:39.623505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create classifier model using transformer layer\n\nTransformer layer outputs one vector for each time step of our input sequence.\nHere, we take the mean across all time steps and\nuse a feed forward network on top of it to classify text.","metadata":{"id":"FZ45fD9j5xqN"}},{"cell_type":"code","source":"embed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(20, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(2, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"id":"fRhVo9Kc5xqO","execution":{"iopub.status.busy":"2023-09-20T03:58:39.629534Z","iopub.execute_input":"2023-09-20T03:58:39.631984Z","iopub.status.idle":"2023-09-20T03:58:44.640703Z","shell.execute_reply.started":"2023-09-20T03:58:39.631948Z","shell.execute_reply":"2023-09-20T03:58:44.639547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Evaluate","metadata":{"id":"J901fDd75xqO"}},{"cell_type":"code","source":"model.compile(\n    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\nhistory = model.fit(\n    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n)","metadata":{"id":"k3WJlKqL5xqO","execution":{"iopub.status.busy":"2023-09-20T03:58:44.641939Z","iopub.execute_input":"2023-09-20T03:58:44.642301Z","iopub.status.idle":"2023-09-20T04:01:08.914991Z","shell.execute_reply.started":"2023-09-20T03:58:44.642269Z","shell.execute_reply":"2023-09-20T04:01:08.913968Z"},"trusted":true},"execution_count":null,"outputs":[]}]}