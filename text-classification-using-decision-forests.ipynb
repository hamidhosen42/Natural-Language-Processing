{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hosen42/text-classification-using-decision-forests?scriptVersionId=143519177\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Text classification using Decision Forests and pretrained embeddings","metadata":{"id":"Bpik16ySmK5i"}},{"cell_type":"markdown","source":"## Introduction\n\n[TensorFlow Decision Forests](https://www.tensorflow.org/decision_forests) (TF-DF)\nis a collection of state-of-the-art algorithms for Decision Forest models that are\ncompatible with Keras APIs. The module includes Random Forests, Gradient Boosted Trees,\nand CART, and can be used for regression, classification, and ranking tasks.\n\nIn this example we will use Gradient Boosted Trees with pretrained embeddings to\nclassify disaster-related tweets.\n\n### See also:\n\n- [TF-DF beginner tutorial](https://www.tensorflow.org/decision_forests/tutorials/beginner_colab)\n- [TF-DF intermediate tutorial](https://www.tensorflow.org/decision_forests/tutorials/intermediate_colab).","metadata":{"id":"7iklYcD9mK5m"}},{"cell_type":"markdown","source":"Install Tensorflow Decision Forest using following command :\n`pip install tensorflow_decision_forests`","metadata":{"id":"yCtXEfYbmK5n"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"rxunLZIomK5o"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport tensorflow_decision_forests as tfdf\nimport matplotlib.pyplot as plt","metadata":{"id":"GIa2vbtymK5p","execution":{"iopub.status.busy":"2023-09-19T11:37:00.712369Z","iopub.execute_input":"2023-09-19T11:37:00.71397Z","iopub.status.idle":"2023-09-19T11:37:13.722429Z","shell.execute_reply.started":"2023-09-19T11:37:00.713892Z","shell.execute_reply":"2023-09-19T11:37:13.720967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the data\n\nThe Dataset is avalaible on [Kaggle](https://www.kaggle.com/c/nlp-getting-started)\n\nDataset description:\n\n**Files:**\n\n- train.csv: the training set\n\n**Columns:**\n\n- id: a unique identifier for each tweet\n- text: the text of the tweet\n- location: the location the tweet was sent from (may be blank)\n- keyword: a particular keyword from the tweet (may be blank)\n- target: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","metadata":{"id":"jHdmAUA5mK5q"}},{"cell_type":"code","source":"# Turn .csv files into pandas DataFrame's\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/IMvision12/Tweets-Classification-NLP/main/train.csv\"\n)\nprint(df.head())","metadata":{"id":"oZ4kmqLHmK5q","execution":{"iopub.status.busy":"2023-09-19T11:37:13.725555Z","iopub.execute_input":"2023-09-19T11:37:13.726885Z","iopub.status.idle":"2023-09-19T11:37:15.4181Z","shell.execute_reply.started":"2023-09-19T11:37:13.726833Z","shell.execute_reply":"2023-09-19T11:37:15.416834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset includes 7613 samples with 5 columns:","metadata":{"id":"rLezLpcOmK5r"}},{"cell_type":"code","source":"print(f\"Training dataset shape: {df.shape}\")","metadata":{"id":"5a5U_AY0mK5r","execution":{"iopub.status.busy":"2023-09-19T11:37:15.420054Z","iopub.execute_input":"2023-09-19T11:37:15.420845Z","iopub.status.idle":"2023-09-19T11:37:15.428854Z","shell.execute_reply.started":"2023-09-19T11:37:15.420801Z","shell.execute_reply":"2023-09-19T11:37:15.427148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shuffling and dropping unnecessary columns:","metadata":{"id":"He5Dfr3xmK5r"}},{"cell_type":"code","source":"df_shuffled = df.sample(frac=1, random_state=42)\n# Dropping id, keyword and location columns as these columns consists of mostly nan values\n# we will be using only text and target columns\ndf_shuffled.drop([\"id\", \"keyword\", \"location\"], axis=1, inplace=True)\ndf_shuffled.reset_index(inplace=True, drop=True)\nprint(df_shuffled.head())","metadata":{"id":"VjOOeA49mK5s","execution":{"iopub.status.busy":"2023-09-19T11:37:15.431984Z","iopub.execute_input":"2023-09-19T11:37:15.432437Z","iopub.status.idle":"2023-09-19T11:37:15.45673Z","shell.execute_reply.started":"2023-09-19T11:37:15.432399Z","shell.execute_reply":"2023-09-19T11:37:15.455767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing information about the shuffled dataframe:","metadata":{"id":"PcbfFPPRmK5s"}},{"cell_type":"code","source":"print(df_shuffled.info())","metadata":{"id":"4Cgy3nG3mK5s","execution":{"iopub.status.busy":"2023-09-19T11:37:15.458259Z","iopub.execute_input":"2023-09-19T11:37:15.458956Z","iopub.status.idle":"2023-09-19T11:37:15.490333Z","shell.execute_reply.started":"2023-09-19T11:37:15.458907Z","shell.execute_reply":"2023-09-19T11:37:15.488765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total number of \"disaster\" and \"non-disaster\" tweets:","metadata":{"id":"0R6IYOFXmK5s"}},{"cell_type":"code","source":"print(\n    \"Total Number of disaster and non-disaster tweets: \"\n    f\"{df_shuffled.target.value_counts()}\"\n)","metadata":{"id":"S-gmU_EfmK5s","execution":{"iopub.status.busy":"2023-09-19T11:37:15.492338Z","iopub.execute_input":"2023-09-19T11:37:15.492797Z","iopub.status.idle":"2023-09-19T11:37:15.501704Z","shell.execute_reply.started":"2023-09-19T11:37:15.492754Z","shell.execute_reply":"2023-09-19T11:37:15.500053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's preview a few samples:","metadata":{"id":"-9anw3xSmK5s"}},{"cell_type":"code","source":"for index, example in df_shuffled[:5].iterrows():\n    print(f\"Example #{index}\")\n    print(f\"\\tTarget : {example['target']}\")\n    print(f\"\\tText : {example['text']}\")","metadata":{"id":"bS5X4qX-mK5s","execution":{"iopub.status.busy":"2023-09-19T11:37:15.503612Z","iopub.execute_input":"2023-09-19T11:37:15.504081Z","iopub.status.idle":"2023-09-19T11:37:15.518991Z","shell.execute_reply.started":"2023-09-19T11:37:15.504036Z","shell.execute_reply":"2023-09-19T11:37:15.517764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting dataset into training and test sets:","metadata":{"id":"0VgGmwn7mK5t"}},{"cell_type":"code","source":"test_df = df_shuffled.sample(frac=0.1, random_state=42)\ntrain_df = df_shuffled.drop(test_df.index)\nprint(f\"Using {len(train_df)} samples for training and {len(test_df)} for validation\")","metadata":{"id":"VvBh7Az6mK5t","execution":{"iopub.status.busy":"2023-09-19T11:37:15.520916Z","iopub.execute_input":"2023-09-19T11:37:15.522312Z","iopub.status.idle":"2023-09-19T11:37:15.536508Z","shell.execute_reply.started":"2023-09-19T11:37:15.522239Z","shell.execute_reply":"2023-09-19T11:37:15.535301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total number of \"disaster\" and \"non-disaster\" tweets in the training data:","metadata":{"id":"nFJlx7drmK5t"}},{"cell_type":"code","source":"print(train_df[\"target\"].value_counts())","metadata":{"id":"vWqvK38xmK5t","execution":{"iopub.status.busy":"2023-09-19T11:37:15.538318Z","iopub.execute_input":"2023-09-19T11:37:15.53974Z","iopub.status.idle":"2023-09-19T11:37:15.549501Z","shell.execute_reply.started":"2023-09-19T11:37:15.539673Z","shell.execute_reply":"2023-09-19T11:37:15.54797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total number of \"disaster\" and \"non-disaster\" tweets in the test data:","metadata":{"id":"dgchHhVJmK5t"}},{"cell_type":"code","source":"print(test_df[\"target\"].value_counts())","metadata":{"id":"wkZOk0mXmK5t","execution":{"iopub.status.busy":"2023-09-19T11:37:15.555085Z","iopub.execute_input":"2023-09-19T11:37:15.556346Z","iopub.status.idle":"2023-09-19T11:37:15.565662Z","shell.execute_reply.started":"2023-09-19T11:37:15.55627Z","shell.execute_reply":"2023-09-19T11:37:15.56439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert data to a `tf.data.Dataset`","metadata":{"id":"knJ0UR18mK5t"}},{"cell_type":"code","source":"\ndef create_dataset(dataframe):\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (dataframe[\"text\"].to_numpy(), dataframe[\"target\"].to_numpy())\n    )\n    dataset = dataset.batch(100)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\n\ntrain_ds = create_dataset(train_df)\ntest_ds = create_dataset(test_df)","metadata":{"id":"05o8K1TvmK5u","execution":{"iopub.status.busy":"2023-09-19T11:37:15.567117Z","iopub.execute_input":"2023-09-19T11:37:15.567575Z","iopub.status.idle":"2023-09-19T11:37:15.706476Z","shell.execute_reply.started":"2023-09-19T11:37:15.567539Z","shell.execute_reply":"2023-09-19T11:37:15.705461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading pretrained embeddings\n\nThe Universal Sentence Encoder embeddings encode text into high-dimensional vectors that can be\nused for text classification, semantic similarity, clustering and other natural language\ntasks. They're trained on a variety of data sources and a variety of tasks. Their input is\nvariable-length English text and their output is a 512 dimensional vector.\n\nTo learn more about these pretrained embeddings, see\n[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4).","metadata":{"id":"Qi5xdn5xmK5u"}},{"cell_type":"code","source":"sentence_encoder_layer = hub.KerasLayer(\n    \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n)","metadata":{"id":"m98lsTK2mK5u","execution":{"iopub.status.busy":"2023-09-19T11:37:15.707821Z","iopub.execute_input":"2023-09-19T11:37:15.70821Z","iopub.status.idle":"2023-09-19T11:37:58.641465Z","shell.execute_reply.started":"2023-09-19T11:37:15.708178Z","shell.execute_reply":"2023-09-19T11:37:58.639806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating our models\n\nWe create two models. In the first model (model_1) raw text will be first encoded via\npretrained embeddings and then passed to a Gradient Boosted Tree model for\nclassification. In the second model (model_2) raw text will be directly passed to\nthe Gradient Boosted Trees model.","metadata":{"id":"QScmZDn3mK5u"}},{"cell_type":"markdown","source":"Building model_1","metadata":{"id":"VceJkud7mK5u"}},{"cell_type":"code","source":"inputs = layers.Input(shape=(), dtype=tf.string)\noutputs = sentence_encoder_layer(inputs)\npreprocessor = keras.Model(inputs=inputs, outputs=outputs)\nmodel_1 = tfdf.keras.GradientBoostedTreesModel(preprocessing=preprocessor)","metadata":{"id":"qBw0FXiMmK5u","execution":{"iopub.status.busy":"2023-09-19T11:37:58.64334Z","iopub.execute_input":"2023-09-19T11:37:58.64386Z","iopub.status.idle":"2023-09-19T11:37:59.903515Z","shell.execute_reply.started":"2023-09-19T11:37:58.643826Z","shell.execute_reply":"2023-09-19T11:37:59.901766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building model_2","metadata":{"id":"EFjgDiWjmK5u"}},{"cell_type":"code","source":"model_2 = tfdf.keras.GradientBoostedTreesModel()","metadata":{"id":"grYIAI-zmK5u","execution":{"iopub.status.busy":"2023-09-19T11:37:59.9065Z","iopub.execute_input":"2023-09-19T11:37:59.906898Z","iopub.status.idle":"2023-09-19T11:38:01.223578Z","shell.execute_reply.started":"2023-09-19T11:37:59.906868Z","shell.execute_reply":"2023-09-19T11:38:01.222168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the models\n\nWe compile our model by passing the metrics `Accuracy`, `Recall`, `Precision` and\n`AUC`. When it comes to the loss, TF-DF automatically detects the best loss for the task\n(Classification or regression). It is printed in the model summary.\n\nAlso, because they're batch-training models rather than mini-batch gradient descent models,\nTF-DF models do not need a validation dataset to monitor overfitting, or to stop\ntraining early. Some algorithms do not use a validation dataset (e.g. Random Forest)\nwhile some others do (e.g. Gradient Boosted Trees). If a validation dataset is\nneeded, it will be extracted automatically from the training dataset.","metadata":{"id":"WTmGTFaWmK5u"}},{"cell_type":"code","source":"# Compiling model_1\nmodel_1.compile(metrics=[\"Accuracy\", \"Recall\", \"Precision\", \"AUC\"])\n# Here we do not specify epochs as, TF-DF trains exactly one epoch of the dataset\nmodel_1.fit(train_ds)\n\n# Compiling model_2\nmodel_2.compile(metrics=[\"Accuracy\", \"Recall\", \"Precision\", \"AUC\"])\n# Here we do not specify epochs as, TF-DF trains exactly one epoch of the dataset\nmodel_2.fit(train_ds)","metadata":{"id":"Y9yT-DOnmK5v","execution":{"iopub.status.busy":"2023-09-19T11:38:01.225911Z","iopub.execute_input":"2023-09-19T11:38:01.2263Z","iopub.status.idle":"2023-09-19T11:39:58.171467Z","shell.execute_reply.started":"2023-09-19T11:38:01.226267Z","shell.execute_reply":"2023-09-19T11:39:58.170438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prints training logs of model_1","metadata":{"id":"4p0HOGgtmK5v"}},{"cell_type":"code","source":"logs_1 = model_1.make_inspector().training_logs()\nprint(logs_1)","metadata":{"id":"-bbItKfDmK5v","execution":{"iopub.status.busy":"2023-09-19T11:39:58.173151Z","iopub.execute_input":"2023-09-19T11:39:58.17449Z","iopub.status.idle":"2023-09-19T11:39:58.191967Z","shell.execute_reply.started":"2023-09-19T11:39:58.174444Z","shell.execute_reply":"2023-09-19T11:39:58.190812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prints training logs of model_2","metadata":{"id":"65c7DQ-ImK5v"}},{"cell_type":"code","source":"logs_2 = model_2.make_inspector().training_logs()\nprint(logs_2)","metadata":{"id":"lxAoApkOmK5v","execution":{"iopub.status.busy":"2023-09-19T11:39:58.19357Z","iopub.execute_input":"2023-09-19T11:39:58.194794Z","iopub.status.idle":"2023-09-19T11:39:58.206981Z","shell.execute_reply.started":"2023-09-19T11:39:58.194742Z","shell.execute_reply":"2023-09-19T11:39:58.205614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model.summary() method prints a variety of information about your decision tree model, including model type, task, input features, and feature importance.","metadata":{"id":"9FfpU0uVmK5v"}},{"cell_type":"code","source":"print(\"model_1 summary: \")\nprint(model_1.summary())\nprint()\nprint(\"model_2 summary: \")\nprint(model_2.summary())","metadata":{"id":"sdJ1P3XBmK5z","execution":{"iopub.status.busy":"2023-09-19T11:39:58.208714Z","iopub.execute_input":"2023-09-19T11:39:58.210192Z","iopub.status.idle":"2023-09-19T11:39:58.263562Z","shell.execute_reply.started":"2023-09-19T11:39:58.210149Z","shell.execute_reply":"2023-09-19T11:39:58.262886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting training metrics","metadata":{"id":"l8Vbs5HTmK5z"}},{"cell_type":"code","source":"\ndef plot_curve(logs):\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Accuracy\")\n\n    plt.subplot(1, 2, 2)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Loss\")\n\n    plt.show()\n\n\nplot_curve(logs_1)\nplot_curve(logs_2)","metadata":{"id":"R9WAGE6QmK5z","execution":{"iopub.status.busy":"2023-09-19T11:39:58.26462Z","iopub.execute_input":"2023-09-19T11:39:58.26502Z","iopub.status.idle":"2023-09-19T11:39:59.230863Z","shell.execute_reply.started":"2023-09-19T11:39:58.264988Z","shell.execute_reply":"2023-09-19T11:39:59.229373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating on test data","metadata":{"id":"lAJQaXS0mK5z"}},{"cell_type":"code","source":"results = model_1.evaluate(test_ds, return_dict=True, verbose=0)\nprint(\"model_1 Evaluation: \\n\")\nfor name, value in results.items():\n    print(f\"{name}: {value:.4f}\")\n\nresults = model_2.evaluate(test_ds, return_dict=True, verbose=0)\nprint(\"model_2 Evaluation: \\n\")\nfor name, value in results.items():\n    print(f\"{name}: {value:.4f}\")","metadata":{"id":"EhWyeEtrmK5z","execution":{"iopub.status.busy":"2023-09-19T11:39:59.233068Z","iopub.execute_input":"2023-09-19T11:39:59.233609Z","iopub.status.idle":"2023-09-19T11:40:03.692011Z","shell.execute_reply.started":"2023-09-19T11:39:59.233566Z","shell.execute_reply":"2023-09-19T11:40:03.690216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting on validation data","metadata":{"id":"Ey_nXLzAmK50"}},{"cell_type":"code","source":"test_df.reset_index(inplace=True, drop=True)\nfor index, row in test_df.iterrows():\n    text = tf.expand_dims(row[\"text\"], axis=0)\n    preds = model_1.predict_step(text)\n    preds = tf.squeeze(tf.round(preds))\n    print(f\"Text: {row['text']}\")\n    print(f\"Prediction: {int(preds)}\")\n    print(f\"Ground Truth : {row['target']}\")\n    if index == 10:\n        break","metadata":{"id":"RrPfjOaEmK50","execution":{"iopub.status.busy":"2023-09-19T11:40:03.693699Z","iopub.execute_input":"2023-09-19T11:40:03.694844Z","iopub.status.idle":"2023-09-19T11:40:05.717297Z","shell.execute_reply.started":"2023-09-19T11:40:03.6948Z","shell.execute_reply":"2023-09-19T11:40:05.716031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Concluding remarks\n\nThe TensorFlow Decision Forests package provides powerful models\nthat work especially well with structured data. In our experiments,\nthe Gradient Boosted Tree model with pretrained embeddings achieved 81.6%\ntest accuracy while the plain Gradient Boosted Tree model had 54.4% accuracy.","metadata":{"id":"dvz7FwSemK50"}}]}