{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hosen42/large-scale-multi-label-text-classification?scriptVersionId=143599225\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Introduction\n\nIn this example, we will build a multi-label text classifier to predict the subject areas\nof arXiv papers from their abstract bodies. This type of classifier can be useful for\nconference submission portals like [OpenReview](https://openreview.net/). Given a paper\nabstract, the portal could provide suggestions for which areas the paper would\nbest belong to.\n\nThe dataset was collected using the\n[`arXiv` Python library](https://github.com/lukasschwab/arxiv.py)\nthat provides a wrapper around the\n[original arXiv API](http://arxiv.org/help/api/index).\nTo learn more about the data collection process, please refer to\n[this notebook](https://github.com/soumik12345/multi-label-text-classification/blob/master/arxiv_scrape.ipynb).\nAdditionally, you can also find the dataset on\n[Kaggle](https://www.kaggle.com/spsayakpaul/arxiv-paper-abstracts).","metadata":{"id":"p7kUryPh215x"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"vJOnvAep215y"}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom ast import literal_eval\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","metadata":{"id":"bRhpKt25215z","execution":{"iopub.status.busy":"2023-09-20T05:32:38.231003Z","iopub.execute_input":"2023-09-20T05:32:38.231939Z","iopub.status.idle":"2023-09-20T05:32:54.16524Z","shell.execute_reply.started":"2023-09-20T05:32:38.231891Z","shell.execute_reply":"2023-09-20T05:32:54.164114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Perform exploratory data analysis\n\nIn this section, we first load the dataset into a `pandas` dataframe and then perform\nsome basic exploratory data analysis (EDA).","metadata":{"id":"98kS4w4V2150"}},{"cell_type":"code","source":"arxiv_data = pd.read_csv(\n    \"https://github.com/soumik12345/multi-label-text-classification/releases/download/v0.2/arxiv_data.csv\"\n)\narxiv_data.head()","metadata":{"id":"yuUKut4Q2151","execution":{"iopub.status.busy":"2023-09-20T05:32:54.16703Z","iopub.execute_input":"2023-09-20T05:32:54.1682Z","iopub.status.idle":"2023-09-20T05:32:55.662653Z","shell.execute_reply.started":"2023-09-20T05:32:54.168163Z","shell.execute_reply":"2023-09-20T05:32:55.661685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our text features are present in the `summaries` column and their corresponding labels\nare in `terms`. As you can notice, there are multiple categories associated with a\nparticular entry.","metadata":{"id":"dFqqMOrc2151"}},{"cell_type":"code","source":"print(f\"There are {len(arxiv_data)} rows in the dataset.\")","metadata":{"id":"-49l-_nl2151","execution":{"iopub.status.busy":"2023-09-20T05:32:55.664488Z","iopub.execute_input":"2023-09-20T05:32:55.665192Z","iopub.status.idle":"2023-09-20T05:32:55.676186Z","shell.execute_reply.started":"2023-09-20T05:32:55.665159Z","shell.execute_reply":"2023-09-20T05:32:55.674966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Real-world data is noisy. One of the most commonly observed source of noise is data\nduplication. Here we notice that our initial dataset has got about 13k duplicate entries.","metadata":{"id":"1mGs7wtM2152"}},{"cell_type":"code","source":"total_duplicate_titles = sum(arxiv_data[\"titles\"].duplicated())\nprint(f\"There are {total_duplicate_titles} duplicate titles.\")","metadata":{"id":"NsIbKJ712152","execution":{"iopub.status.busy":"2023-09-20T05:32:55.679421Z","iopub.execute_input":"2023-09-20T05:32:55.679764Z","iopub.status.idle":"2023-09-20T05:32:55.732871Z","shell.execute_reply.started":"2023-09-20T05:32:55.679718Z","shell.execute_reply":"2023-09-20T05:32:55.730597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before proceeding further, we drop these entries.","metadata":{"id":"TaDP9DMz2153"}},{"cell_type":"code","source":"arxiv_data = arxiv_data[~arxiv_data[\"titles\"].duplicated()]\nprint(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n\n# There are some terms with occurrence as low as 1.\nprint(sum(arxiv_data[\"terms\"].value_counts() == 1))\n\n# How many unique terms?\nprint(arxiv_data[\"terms\"].nunique())","metadata":{"id":"Gf02_TJY2153","execution":{"iopub.status.busy":"2023-09-20T05:32:55.73632Z","iopub.execute_input":"2023-09-20T05:32:55.736616Z","iopub.status.idle":"2023-09-20T05:32:55.792891Z","shell.execute_reply.started":"2023-09-20T05:32:55.736589Z","shell.execute_reply":"2023-09-20T05:32:55.791905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As observed above, out of 3,157 unique combinations of `terms`, 2,321 entries have the\nlowest occurrence. To prepare our train, validation, and test sets with\n[stratification](https://en.wikipedia.org/wiki/Stratified_sampling), we need to drop\nthese terms.","metadata":{"id":"2XCCTY4f2153"}},{"cell_type":"code","source":"# Filtering the rare terms.\narxiv_data_filtered = arxiv_data.groupby(\"terms\").filter(lambda x: len(x) > 1)\narxiv_data_filtered.shape","metadata":{"id":"3GQJ18bF2154","execution":{"iopub.status.busy":"2023-09-20T05:32:55.794383Z","iopub.execute_input":"2023-09-20T05:32:55.794726Z","iopub.status.idle":"2023-09-20T05:32:55.924512Z","shell.execute_reply.started":"2023-09-20T05:32:55.794693Z","shell.execute_reply":"2023-09-20T05:32:55.923545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert the string labels to lists of strings\n\nThe initial labels are represented as raw strings. Here we make them `List[str]` for a\nmore compact representation.","metadata":{"id":"mwBPIoTy2154"}},{"cell_type":"code","source":"arxiv_data_filtered[\"terms\"] = arxiv_data_filtered[\"terms\"].apply(\n    lambda x: literal_eval(x)\n)\narxiv_data_filtered[\"terms\"].values[:5]","metadata":{"id":"ghqOhDnh2154","execution":{"iopub.status.busy":"2023-09-20T05:32:55.926007Z","iopub.execute_input":"2023-09-20T05:32:55.926433Z","iopub.status.idle":"2023-09-20T05:32:56.332723Z","shell.execute_reply.started":"2023-09-20T05:32:55.926398Z","shell.execute_reply":"2023-09-20T05:32:56.331851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use stratified splits because of class imbalance\n\nThe dataset has a\n[class imbalance problem](https://developers.google.com/machine-learning/glossary/#class-imbalanced-dataset).\nSo, to have a fair evaluation result, we need to ensure the datasets are sampled with\nstratification. To know more about different strategies to deal with the class imbalance\nproblem, you can follow\n[this tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data).\nFor an end-to-end demonstration of classification with imbablanced data, refer to\n[Imbalanced classification: credit card fraud detection](https://keras.io/examples/structured_data/imbalanced_classification/).","metadata":{"id":"aRaIbfEu2154"}},{"cell_type":"code","source":"test_split = 0.1\n\n# Initial train and test split.\ntrain_df, test_df = train_test_split(\n    arxiv_data_filtered,\n    test_size=test_split,\n    stratify=arxiv_data_filtered[\"terms\"].values,\n)\n\n# Splitting the test set further into validation\n# and new test sets.\nval_df = test_df.sample(frac=0.5)\ntest_df.drop(val_df.index, inplace=True)\n\nprint(f\"Number of rows in training set: {len(train_df)}\")\nprint(f\"Number of rows in validation set: {len(val_df)}\")\nprint(f\"Number of rows in test set: {len(test_df)}\")","metadata":{"id":"o0FyWdW52154","execution":{"iopub.status.busy":"2023-09-20T05:32:56.334256Z","iopub.execute_input":"2023-09-20T05:32:56.334604Z","iopub.status.idle":"2023-09-20T05:32:56.489516Z","shell.execute_reply.started":"2023-09-20T05:32:56.334571Z","shell.execute_reply":"2023-09-20T05:32:56.487616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multi-label binarization\n\nNow we preprocess our labels using the\n[`StringLookup`](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup)\nlayer.","metadata":{"id":"8UWdNLGR2155"}},{"cell_type":"code","source":"terms = tf.ragged.constant(train_df[\"terms\"].values)\nlookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\nlookup.adapt(terms)\nvocab = lookup.get_vocabulary()\n\n\ndef invert_multi_hot(encoded_labels):\n    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n    return np.take(vocab, hot_indices)\n\n\nprint(\"Vocabulary:\\n\")\nprint(vocab)\n","metadata":{"id":"yIlmarY82155","execution":{"iopub.status.busy":"2023-09-20T05:32:56.491066Z","iopub.execute_input":"2023-09-20T05:32:56.491435Z","iopub.status.idle":"2023-09-20T05:33:05.06889Z","shell.execute_reply.started":"2023-09-20T05:32:56.491402Z","shell.execute_reply":"2023-09-20T05:33:05.067937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are separating the individual unique classes available from the label\npool and then using this information to represent a given label set with 0's and 1's.\nBelow is an example.","metadata":{"id":"PKBIy5sa2155"}},{"cell_type":"code","source":"sample_label = train_df[\"terms\"].iloc[0]\nprint(f\"Original label: {sample_label}\")\n\nlabel_binarized = lookup([sample_label])\nprint(f\"Label-binarized representation: {label_binarized}\")","metadata":{"id":"uNABHEwi2155","execution":{"iopub.status.busy":"2023-09-20T05:33:05.073226Z","iopub.execute_input":"2023-09-20T05:33:05.073526Z","iopub.status.idle":"2023-09-20T05:33:05.148163Z","shell.execute_reply.started":"2023-09-20T05:33:05.073499Z","shell.execute_reply":"2023-09-20T05:33:05.147067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing and `tf.data.Dataset` objects\n\nWe first get percentile estimates of the sequence lengths. The purpose will be clear in a\nmoment.","metadata":{"id":"U87jBpix2156"}},{"cell_type":"code","source":"train_df[\"summaries\"].apply(lambda x: len(x.split(\" \"))).describe()","metadata":{"id":"BJ-xvcO12156","execution":{"iopub.status.busy":"2023-09-20T05:33:05.149676Z","iopub.execute_input":"2023-09-20T05:33:05.150358Z","iopub.status.idle":"2023-09-20T05:33:05.53301Z","shell.execute_reply.started":"2023-09-20T05:33:05.150307Z","shell.execute_reply":"2023-09-20T05:33:05.532048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that 50% of the abstracts have a length of 154 (you may get a different number\nbased on the split). So, any number close to that value is a good enough approximate for the\nmaximum sequence length.\n\nNow, we implement utilities to prepare our datasets.","metadata":{"id":"95aJmHLb2156"}},{"cell_type":"code","source":"max_seqlen = 150\nbatch_size = 128\npadding_token = \"<pad>\"\nauto = tf.data.AUTOTUNE\n\n\ndef make_dataset(dataframe, is_train=True):\n    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n    label_binarized = lookup(labels).numpy()\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (dataframe[\"summaries\"].values, label_binarized)\n    )\n    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n    return dataset.batch(batch_size)\n","metadata":{"id":"IPYPGExM2156","execution":{"iopub.status.busy":"2023-09-20T05:33:05.534382Z","iopub.execute_input":"2023-09-20T05:33:05.534843Z","iopub.status.idle":"2023-09-20T05:33:05.542298Z","shell.execute_reply.started":"2023-09-20T05:33:05.534806Z","shell.execute_reply":"2023-09-20T05:33:05.541077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can prepare the `tf.data.Dataset` objects.","metadata":{"id":"oSwCxYk32157"}},{"cell_type":"code","source":"train_dataset = make_dataset(train_df, is_train=True)\nvalidation_dataset = make_dataset(val_df, is_train=False)\ntest_dataset = make_dataset(test_df, is_train=False)","metadata":{"id":"2_6gAN4q2157","execution":{"iopub.status.busy":"2023-09-20T05:33:05.544054Z","iopub.execute_input":"2023-09-20T05:33:05.544853Z","iopub.status.idle":"2023-09-20T05:33:06.068058Z","shell.execute_reply.started":"2023-09-20T05:33:05.544816Z","shell.execute_reply":"2023-09-20T05:33:06.067051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset preview","metadata":{"id":"ncPd6YpT2157"}},{"cell_type":"code","source":"text_batch, label_batch = next(iter(train_dataset))\n\nfor i, text in enumerate(text_batch[:5]):\n    label = label_batch[i].numpy()[None, ...]\n    print(f\"Abstract: {text}\")\n    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n    print(\" \")","metadata":{"id":"Spaycqkb2157","execution":{"iopub.status.busy":"2023-09-20T05:33:06.069453Z","iopub.execute_input":"2023-09-20T05:33:06.069866Z","iopub.status.idle":"2023-09-20T05:33:06.163812Z","shell.execute_reply.started":"2023-09-20T05:33:06.069817Z","shell.execute_reply":"2023-09-20T05:33:06.162914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorization\n\nBefore we feed the data to our model, we need to vectorize it (represent it in a numerical form).\nFor that purpose, we will use the\n[`TextVectorization` layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization).\nIt can operate as a part of your main model so that the model is excluded from the core\npreprocessing logic. This greatly reduces the chances of training / serving skew during inference.\n\nWe first calculate the number of unique words present in the abstracts.","metadata":{"id":"nsXHSjG12157"}},{"cell_type":"code","source":"# Source: https://stackoverflow.com/a/18937309/7636462\nvocabulary = set()\ntrain_df[\"summaries\"].str.lower().str.split().apply(vocabulary.update)\nvocabulary_size = len(vocabulary)\nprint(vocabulary_size)\n","metadata":{"id":"cqMP0Z6F2157","execution":{"iopub.status.busy":"2023-09-20T05:33:06.165456Z","iopub.execute_input":"2023-09-20T05:33:06.165797Z","iopub.status.idle":"2023-09-20T05:33:07.4054Z","shell.execute_reply.started":"2023-09-20T05:33:06.165762Z","shell.execute_reply":"2023-09-20T05:33:07.404411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now create our vectorization layer and `map()` to the `tf.data.Dataset`s created\nearlier.","metadata":{"id":"OgUB1kKe2158"}},{"cell_type":"code","source":"text_vectorizer = layers.TextVectorization(\n    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n)\n\n# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n# training set.\nwith tf.device(\"/CPU:0\"):\n    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n\ntrain_dataset = train_dataset.map(\n    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n).prefetch(auto)\nvalidation_dataset = validation_dataset.map(\n    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n).prefetch(auto)\ntest_dataset = test_dataset.map(\n    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n).prefetch(auto)\n","metadata":{"id":"m1drSHpT2158","execution":{"iopub.status.busy":"2023-09-20T05:33:07.406689Z","iopub.execute_input":"2023-09-20T05:33:07.407789Z","iopub.status.idle":"2023-09-20T05:33:20.910283Z","shell.execute_reply.started":"2023-09-20T05:33:07.407752Z","shell.execute_reply":"2023-09-20T05:33:20.908941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A batch of raw text will first go through the `TextVectorization` layer and it will\ngenerate their integer representations. Internally, the `TextVectorization` layer will\nfirst create bi-grams out of the sequences and then represent them using\n[TF-IDF](https://wikipedia.org/wiki/Tf%E2%80%93idf). The output representations will then\nbe passed to the shallow model responsible for text classification.\n\nTo learn more about other possible configurations with `TextVectorizer`, please consult\nthe\n[official documentation](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization).\n\n**Note**: Setting the `max_tokens` argument to a pre-calculated vocabulary size is\nnot a requirement.","metadata":{"id":"LinZJ_bd2158"}},{"cell_type":"markdown","source":"## Create a text classification model\n\nWe will keep our model simple -- it will be a small stack of fully-connected layers with\nReLU as the non-linearity.","metadata":{"id":"Z5JUfpvn2158"}},{"cell_type":"code","source":"\ndef make_model():\n    shallow_mlp_model = keras.Sequential(\n        [\n            layers.Dense(512, activation=\"relu\"),\n            layers.Dense(256, activation=\"relu\"),\n            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n        ]  # More on why \"sigmoid\" has been used here in a moment.\n    )\n    return shallow_mlp_model\n","metadata":{"id":"mmTutBLM2158","execution":{"iopub.status.busy":"2023-09-20T05:33:20.91513Z","iopub.execute_input":"2023-09-20T05:33:20.916058Z","iopub.status.idle":"2023-09-20T05:33:20.925253Z","shell.execute_reply.started":"2023-09-20T05:33:20.916021Z","shell.execute_reply":"2023-09-20T05:33:20.924028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model\n\nWe will train our model using the binary crossentropy loss. This is because the labels\nare not disjoint. For a given abstract, we may have multiple categories. So, we will\ndivide the prediction task into a series of multiple binary classification problems. This\nis also why we kept the activation function of the classification layer in our model to\nsigmoid. Researchers have used other combinations of loss function and activation\nfunction as well. For example, in [Exploring the Limits of Weakly Supervised Pretraining](https://arxiv.org/abs/1805.00932),\nMahajan et al. used the softmax activation function and cross-entropy loss to train\ntheir models.\n\nThere are several options of metrics that can be used in multi-label classification.\nTo keep this code example narrow we decided to use the\n[binary accuracy metric](https://keras.io/api/metrics/accuracy_metrics/#binaryaccuracy-class).\nTo see the explanation why this metric is used we refer to this\n[pull-request](https://github.com/keras-team/keras-io/pull/1133#issuecomment-1322736860).\nThere are also other suitable metrics for multi-label classification, like\n[F1 Score](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score) or\n[Hamming loss](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/HammingLoss).","metadata":{"id":"aj6m1_k42158"}},{"cell_type":"code","source":"epochs = 20\n\nshallow_mlp_model = make_model()\nshallow_mlp_model.compile(\n    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_accuracy\"]\n)\n\nhistory = shallow_mlp_model.fit(\n    train_dataset, validation_data=validation_dataset, epochs=epochs\n)\n\n\ndef plot_result(item):\n    plt.plot(history.history[item], label=item)\n    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(item)\n    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n\nplot_result(\"loss\")\nplot_result(\"binary_accuracy\")","metadata":{"id":"kIJnlsir2159","execution":{"iopub.status.busy":"2023-09-20T05:33:20.926995Z","iopub.execute_input":"2023-09-20T05:33:20.927741Z","iopub.status.idle":"2023-09-20T05:42:48.232914Z","shell.execute_reply.started":"2023-09-20T05:33:20.927705Z","shell.execute_reply":"2023-09-20T05:42:48.231956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While training, we notice an initial sharp fall in the loss followed by a gradual decay.","metadata":{"id":"7Aw9HLbe215-"}},{"cell_type":"markdown","source":"### Evaluate the model","metadata":{"id":"MM9TEfG2215-"}},{"cell_type":"code","source":"_, binary_acc = shallow_mlp_model.evaluate(test_dataset)\nprint(f\"Categorical accuracy on the test set: {round(binary_acc * 100, 2)}%.\")","metadata":{"id":"lnOgGY5y215-","execution":{"iopub.status.busy":"2023-09-20T05:42:48.236914Z","iopub.execute_input":"2023-09-20T05:42:48.24116Z","iopub.status.idle":"2023-09-20T05:42:49.966985Z","shell.execute_reply.started":"2023-09-20T05:42:48.241115Z","shell.execute_reply":"2023-09-20T05:42:49.965589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The trained model gives us an evaluation accuracy of ~99%.","metadata":{"id":"GZxgT7yH215_"}},{"cell_type":"markdown","source":"## Inference\n\nAn important feature of the\n[preprocessing layers provided by Keras](https://keras.io/guides/preprocessing_layers/)\nis that they can be included inside a `tf.keras.Model`. We will export an inference model\nby including the `text_vectorization` layer on top of `shallow_mlp_model`. This will\nallow our inference model to directly operate on raw strings.\n\n**Note** that during training it is always preferable to use these preprocessing\nlayers as a part of the data input pipeline rather than the model to avoid\nsurfacing bottlenecks for the hardware accelerators. This also allows for\nasynchronous data processing.","metadata":{"id":"lVKyEn6i215_"}},{"cell_type":"code","source":"# Create a model for inference.\nmodel_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])\n\n# Create a small dataset just for demoing inference.\ninference_dataset = make_dataset(test_df.sample(100), is_train=False)\ntext_batch, label_batch = next(iter(inference_dataset))\npredicted_probabilities = model_for_inference.predict(text_batch)\n\n# Perform inference.\nfor i, text in enumerate(text_batch[:5]):\n    label = label_batch[i].numpy()[None, ...]\n    print(f\"Abstract: {text}\")\n    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n    predicted_proba = [proba for proba in predicted_probabilities[i]]\n    top_3_labels = [\n        x\n        for _, x in sorted(\n            zip(predicted_probabilities[i], lookup.get_vocabulary()),\n            key=lambda pair: pair[0],\n            reverse=True,\n        )\n    ][:3]\n    print(f\"Predicted Label(s): ({', '.join([label for label in top_3_labels])})\")\n    print(\" \")","metadata":{"id":"hr-C_OOU215_","execution":{"iopub.status.busy":"2023-09-20T05:42:49.968283Z","iopub.execute_input":"2023-09-20T05:42:49.968648Z","iopub.status.idle":"2023-09-20T05:42:50.429597Z","shell.execute_reply.started":"2023-09-20T05:42:49.968613Z","shell.execute_reply":"2023-09-20T05:42:50.428597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The prediction results are not that great but not below the par for a simple model like\nours. We can improve this performance with models that consider word order like LSTM or\neven those that use Transformers ([Vaswani et al.](https://arxiv.org/abs/1706.03762)).","metadata":{"id":"-6zEPTm_215_"}}]}